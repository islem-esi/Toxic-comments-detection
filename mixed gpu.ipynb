{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os, time\nimport pandas\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom kaggle_datasets import KaggleDatasets\nprint(tf.version.VERSION)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQUENCE_LENGTH = 192\nGCS_PATH = KaggleDatasets().get_gcs_path('mixed192')\n#TST_PATH = KaggleDatasets().get_gcs_path('test384')\n#VAL_PATH = KaggleDatasets().get_gcs_path('val384')\nBERT_GCS_PATH = KaggleDatasets().get_gcs_path('bert-multilanguage')\nBERT_GCS_PATH_SAVEDMODEL = BERT_GCS_PATH ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multilingual_bert_model(max_seq_length=SEQUENCE_LENGTH, trainable_bert=True):\n    \"\"\"Build and return a multilingual BERT model and tokenizer.\"\"\"\n    input_word_ids = tf.keras.layers.Input(\n        shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(\n        shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(\n        shape=(max_seq_length,), dtype=tf.int32, name=\"all_segment_id\")\n    \n    # Load a SavedModel on TPU from GCS. This model is available online at \n    # https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1. You can use your own \n    # pretrained models, but will need to add them as a Kaggle dataset.\n    bert_layer = tf.saved_model.load(BERT_GCS_PATH_SAVEDMODEL)\n    # Cast the loaded model to a TFHub KerasLayer.\n    bert_layer = hub.KerasLayer(bert_layer, trainable=trainable_bert)\n\n    pooled_output, _ = bert_layer([input_word_ids, input_mask, segment_ids])\n    #output = tf.keras.layers.Dense(728, activation='relu')(pooled_output)\n    outputs = []\n    for _ in range(8):\n        outputs.append(tf.keras.layers.Dense(32, activation='relu', kernel_initializer = 'random_normal')(pooled_output))\n    output = tf.keras.layers.concatenate(outputs)\n    output = tf.keras.layers.Dense(16, activation='relu', kernel_initializer = 'random_normal')(output)\n    output = tf.keras.layers.Dense(1, activation='sigmoid', name='labels', kernel_initializer = 'random_normal')(output)\n\n    return tf.keras.Model(inputs={'input_word_ids': input_word_ids,\n                                  'input_mask': input_mask,\n                                  'all_segment_id': segment_ids},\n                          outputs=output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multilingual_bert = multilingual_bert_model()\nmultilingual_bert.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n    metrics=[tf.keras.metrics.AUC()])\n\nmultilingual_bert.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pandas.read_csv(GCS_PATH+'/mixed-processed-seqlen192.csv', usecols=['input_word_ids', 'input_mask', 'all_segment_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.array([1. for _ in range(500000)]+[0. for _ in range(500000)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['labels']=labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = dataset.pop('labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_data = tf.data.Dataset.from_tensor_slices(dataset['input_word_ids'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in range(1):\n    history = multilingual_bert.fit(\n        # Set steps such that the number of examples per epoch is fixed.\n        # This makes training on different accelerators more comparable.\n        tf_data,\n        epochs=3, steps_per_epoch=500)\n    print()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}